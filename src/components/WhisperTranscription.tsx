'use client';

import { useState, useEffect, useRef, useCallback } from 'react';

interface TranscriptionEntry {
  id: string;
  text: string;
  timestamp: Date;
  speaker?: string;
}

interface WhisperTranscriptionProps {
  onTranscription?: (entry: TranscriptionEntry) => void;
  onError?: (error: string) => void;
  onAudioSourceChange?: (source: string, level: number) => void;
  apiKey?: string;
  meetingType?: 'in-person' | 'online';
}

export function useWhisperTranscription({ onTranscription, onError, onAudioSourceChange, apiKey, meetingType }: WhisperTranscriptionProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [isSupported, setIsSupported] = useState(false);
  const streamRef = useRef<MediaStream | null>(null);
  const intervalRef = useRef<NodeJS.Timeout | null>(null);
  const currentRecorderRef = useRef<MediaRecorder | null>(null);
  const isPausedRef = useRef<boolean>(false);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const audioLevelRef = useRef<number>(0);
  const genericResponseCountRef = useRef<number>(0);
  const lastTranscriptionTimeRef = useRef<number>(0);

  useEffect(() => {
    setIsSupported(true);
    
    return () => {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, []);

  // éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–ã®è¨­å®š
  const setupAudioAnalysis = useCallback((stream: MediaStream) => {
    try {
      const audioContext = new (window.AudioContext || (window as unknown as { webkitAudioContext: typeof AudioContext }).webkitAudioContext)();
      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(stream);
      
      analyser.fftSize = 256;
      source.connect(analyser);
      
      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      
      console.log('[WHISPER] Audio analysis setup complete');
      
      // éŸ³å£°ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
      const trackLabel = stream.getAudioTracks()[0]?.label || '';
      
      // éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–ãƒ«ãƒ¼ãƒ—
      const monitorAudioLevel = () => {
        if (analyserRef.current) {
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          analyser.getByteFrequencyData(dataArray);
          
          // å¹³å‡éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
          const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
          audioLevelRef.current = average;
          
          // éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’UIã«é€ä¿¡ï¼ˆ1ç§’ã«ç´„2å›ç¨‹åº¦ï¼‰
          if (Math.random() < 0.02 && onAudioSourceChange) {
            onAudioSourceChange(trackLabel, average);
          }
          
          // éŸ³å£°ãƒ¬ãƒ™ãƒ«ãŒç¶™ç¶šçš„ã«0ã®å ´åˆã®è­¦å‘Š
          if (average < 1) {
            const now = Date.now();
            if (now - lastTranscriptionTimeRef.current > 60000) { // 1åˆ†é–“éŸ³å£°ãªã—
              if (Math.random() < 0.001) { // ç´„0.1%ã®ç¢ºç‡ï¼ˆ10ç§’ã«1å›ç¨‹åº¦ï¼‰
                const isInPerson = meetingType === 'in-person';
                if (isInPerson) {
                  console.warn('[WHISPER] ğŸ”´ ãƒã‚¤ã‚¯éŸ³å£°ãƒ¬ãƒ™ãƒ«ãŒ0ã§ã™');
                  console.warn('[WHISPER] ãƒã‚¤ã‚¯è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„:');
                  console.warn('[WHISPER] 1. ãƒã‚¤ã‚¯ãŒæ­£ã—ãæ¥ç¶šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª');
                  console.warn('[WHISPER] 2. ãƒã‚¤ã‚¯ã®éŸ³é‡ã‚’ä¸Šã’ã¦ãã ã•ã„');
                  console.warn('[WHISPER] 3. å®Ÿéš›ã«å£°ã‚’å‡ºã—ã¦è©±ã—ã¦ãã ã•ã„');
                } else {
                  console.warn('[WHISPER] ğŸ”´ BlackHoleãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ãŒéŸ³å£°ãƒ¬ãƒ™ãƒ«ãŒ0ã§ã™');
                  console.warn('[WHISPER] ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„:');
                  console.warn('[WHISPER] 1. ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨­å®š > ã‚µã‚¦ãƒ³ãƒ‰ > å‡ºåŠ›ã§ã€ŒMulti-Output Deviceã€ã‚’é¸æŠ');
                  console.warn('[WHISPER] 2. ã¾ãŸã¯éŸ³å£°è¨­å®šã§ã€Œç”»é¢å…±æœ‰ã€ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ');
                }
              }
            }
          }
          
          // é€šå¸¸ã®éŸ³å£°ãƒ¬ãƒ™ãƒ«ãƒ­ã‚°ï¼ˆ5ç§’ã”ã¨ï¼‰
          if (Math.random() < 0.01) { // ç´„1%ã®ç¢ºç‡ã§ãƒ­ã‚°å‡ºåŠ›ï¼ˆ5ç§’ã«1å›ç¨‹åº¦ï¼‰
            console.log('[WHISPER] Audio level:', Math.round(average), 'Max:', Math.max(...dataArray));
          }
        }
        
        if (isRecording) {
          requestAnimationFrame(monitorAudioLevel);
        }
      };
      
      monitorAudioLevel();
      
    } catch (error) {
      console.error('[WHISPER] Audio analysis setup failed:', error);
    }
  }, [isRecording, onAudioSourceChange, meetingType]);

  // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ
  const analyzeAudioBlob = useCallback(async (audioBlob: Blob): Promise<void> => {
    console.log('[WHISPER] === AUDIO ANALYSIS START ===');
    console.log('[WHISPER] Meeting type:', meetingType);
    console.log('[WHISPER] Blob size:', audioBlob.size, 'bytes');
    console.log('[WHISPER] Blob type:', audioBlob.type);
    console.log('[WHISPER] Current audio level:', Math.round(audioLevelRef.current));
    
    // ArrayBufferã«å¤‰æ›ã—ã¦æœ€åˆã¨æœ€å¾Œã®æ•°ãƒã‚¤ãƒˆã‚’ç¢ºèª
    try {
      const arrayBuffer = await audioBlob.arrayBuffer();
      const uint8Array = new Uint8Array(arrayBuffer);
      
      console.log('[WHISPER] First 10 bytes:', Array.from(uint8Array.slice(0, 10)).map(b => b.toString(16).padStart(2, '0')).join(' '));
      console.log('[WHISPER] Last 10 bytes:', Array.from(uint8Array.slice(-10)).map(b => b.toString(16).padStart(2, '0')).join(' '));
      
      // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ
      const nonZeroBytes = uint8Array.filter(b => b !== 0).length;
      const silenceRatio = (uint8Array.length - nonZeroBytes) / uint8Array.length;
      
      console.log('[WHISPER] Non-zero bytes:', nonZeroBytes, '/', uint8Array.length);
      console.log('[WHISPER] Silence ratio:', Math.round(silenceRatio * 100) + '%');
      
      if (silenceRatio > 0.9) {
        console.warn('[WHISPER] âš ï¸ HIGH SILENCE RATIO - Audio might be too quiet!');
        console.warn('[WHISPER] ğŸ’¡ TIPS:');
        console.warn('[WHISPER]   1. Make sure system audio is actually playing');
        console.warn('[WHISPER]   2. When browser asks for screen sharing, check "Share audio" option');
        console.warn('[WHISPER]   3. Select the correct application/tab that is playing audio');
        console.warn('[WHISPER]   4. Increase system volume if audio is too quiet');
      }
      
      if (audioLevelRef.current < 3) {
        const trackLabel = streamRef.current?.getAudioTracks()[0]?.label || '';
        const isInPerson = meetingType === 'in-person';
        console.warn('[WHISPER] âš ï¸ VERY LOW AUDIO LEVEL:', Math.round(audioLevelRef.current));
        console.warn('[WHISPER] Meeting type:', meetingType, 'Track label:', trackLabel);
        
        if (isInPerson) {
          console.warn('[WHISPER] âš ï¸ Low microphone level detected!');
          console.warn('[WHISPER] Tips for microphone:');
          console.warn('[WHISPER] 1. Check microphone permissions in browser/system');
          console.warn('[WHISPER] 2. Increase microphone volume in system settings');
          console.warn('[WHISPER] 3. Speak closer to the microphone');
          console.warn('[WHISPER] 4. Make sure microphone is not muted');
          console.warn('[WHISPER] 5. Try speaking more loudly to test microphone sensitivity');
        } else {
          console.warn('[WHISPER] This is expected for BlackHole/Virtual devices when no system audio is playing');
          console.warn('[WHISPER] Make sure there is actual system audio playing (music, video, etc.)');
        }
      }
      
      // éŸ³å£°ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¨˜éŒ²ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯å‰Šé™¤ï¼‰
      console.log('[WHISPER] Audio blob analysis completed');
      
    } catch (error) {
      console.error('[WHISPER] Audio analysis failed:', error);
    }
    
    console.log('[WHISPER] === AUDIO ANALYSIS END ===');
  }, [meetingType]);

  const transcribeAudio = useCallback(async (audioBlob: Blob) => {
    console.log('[WHISPER] transcribeAudio called with blob size:', audioBlob.size);
    
    // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æã‚’å®Ÿè¡Œ
    await analyzeAudioBlob(audioBlob);
    
    if (!apiKey) {
      console.error('[WHISPER] No API key available');
      if (onError) onError('OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã‹ã‚‰è¿½åŠ ã—ã¦ãã ã•ã„ã€‚');
      return;
    }

    try {
      const formData = new FormData();
      // é€ä¿¡ã™ã‚‹éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã«æ­£ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã¨MIMEã‚¿ã‚¤ãƒ—ã‚’è¨­å®š
      const fileName = 'audio.webm';
      
      formData.append('file', audioBlob, fileName);
      formData.append('model', 'whisper-1');
      formData.append('language', 'ja');
      formData.append('response_format', 'json');
      
      console.log('[WHISPER] Sending request to Whisper API with data:', {
        fileSize: audioBlob.size,
        fileName,
        model: 'whisper-1',
        language: 'ja'
      });

      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
        },
        body: formData,
      });
      
      console.log('[WHISPER] API response status:', response.status);

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`API Error ${response.status}:`, errorText);
        
        if (response.status === 401) {
          throw new Error('APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚æ­£ã—ã„OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚');
        } else if (response.status === 429) {
          throw new Error('APIä½¿ç”¨é‡ã®åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚');
        } else if (response.status === 400) {
          throw new Error(`éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ${audioBlob.size}ãƒã‚¤ãƒˆ`);
        } else {
          throw new Error(`API ã‚¨ãƒ©ãƒ¼: ${response.status} - ${errorText}`);
        }
      }

      const result = await response.json();
      console.log('[WHISPER] API response result:', result);
      
      if (result.text && result.text.trim()) {
        const transcribedText = result.text.trim();
        
        // å…¸å‹çš„ãªã€Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€å¿œç­”ã‚’æ¤œå‡ºï¼ˆéŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã®å•é¡Œã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®ã¿ï¼‰
        const genericResponses = [
          'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
          'Thank you for watching',
          'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ'
        ];
        
        const isGenericResponse = genericResponses.some(generic => transcribedText === generic);
        
        if (isGenericResponse) {
          genericResponseCountRef.current++;
          console.log('[WHISPER] Generic response detected:', transcribedText, `(${genericResponseCountRef.current} consecutive)`);
          
          // ã‚ˆã‚Šå¯›å®¹ãªæ¡ä»¶ã«å¤‰æ›´ï¼š5å›é€£ç¶š ã‹ã¤ æœ€ä½30ç§’é–“ã®æ²ˆé»™
          const timeSinceLastValid = Date.now() - lastTranscriptionTimeRef.current;
          if (genericResponseCountRef.current >= 5 && timeSinceLastValid > 30000) {
            const isInPerson = meetingType === 'in-person';
            console.warn(`[WHISPER] 5é€£ç¶šã§Generic Response (${Math.round(timeSinceLastValid/1000)}ç§’é–“) - éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã®å•é¡Œã®å¯èƒ½æ€§`);
            if (onError) {
              if (isInPerson) {
                onError('âš ï¸ ãƒã‚¤ã‚¯éŸ³å£°ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\nğŸ”§ ç¢ºèªäº‹é …:\nâ€¢ ãƒã‚¤ã‚¯ãŒæ­£ã—ãæ¥ç¶šã•ã‚Œã¦ã„ã‚‹ã‹\nâ€¢ ãƒã‚¤ã‚¯ã®éŸ³é‡è¨­å®š\nâ€¢ å®Ÿéš›ã«å£°ã‚’å‡ºã—ã¦è©±ã—ã¦ãã ã•ã„');
              } else {
                onError('âš ï¸ ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\nğŸ”§ ç¢ºèªäº‹é …:\nâ€¢ BlackHoleè¨­å®šã®ç¢ºèª\nâ€¢ ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ãŒå®Ÿéš›ã«å†ç”Ÿã•ã‚Œã¦ã„ã‚‹ã‹\nâ€¢ Multi-Output Deviceè¨­å®šã®ç¢ºèª');
              }
            }
            // ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã®ã¿ã§éŒ²éŸ³ã¯ç¶™ç¶š
          }
        } else {
          // æ­£å¸¸ãªå¿œç­”ãŒå¾—ã‚‰ã‚ŒãŸå ´åˆã¯ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
          if (transcribedText.length > 3) { // çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–
            genericResponseCountRef.current = 0;
            lastTranscriptionTimeRef.current = Date.now(); // æ­£å¸¸ãªè»¢å†™æ™‚åˆ»ã‚’è¨˜éŒ²
          }
        }
        
        if (onTranscription) {
        const entry: TranscriptionEntry = {
          id: Date.now().toString(),
            text: transcribedText,
          timestamp: new Date(),
          speaker: 'è©±è€…'
        };
          console.log('[WHISPER] Transcription result:', entry.text);
        onTranscription(entry);
        }
      } else {
        console.log('[WHISPER] No transcription text received or text is empty');
      }
    } catch (error) {
      console.error('[WHISPER] Transcription error:', error);
      if (onError) {
        onError(error instanceof Error ? error.message : 'Whisper APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ');
      }
    }
  }, [apiKey, onTranscription, onError, analyzeAudioBlob, meetingType]);

  const startRecording = useCallback(async () => {
    if (!isSupported || isRecording) return false;
    
    const isInPerson = meetingType === 'in-person';
    console.log(`[WHISPER] Starting recording for ${isInPerson ? 'in-person' : 'online'} meeting...`);

    try {
      let stream = null;
      
      if (isInPerson) {
        // å¯¾é¢ä¼šè­°ï¼šå®Ÿéš›ã®ãƒã‚¤ã‚¯ãƒ‡ãƒã‚¤ã‚¹ã‚’æ˜ç¤ºçš„ã«é¸æŠ
        console.log('[WHISPER] Selecting real microphone for in-person meeting');
        
        // åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ã‚’åˆ—æŒ™
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioInputs = devices.filter(device => device.kind === 'audioinput');
        
        console.log('[WHISPER] Available audio input devices:', audioInputs.map(d => ({
          deviceId: d.deviceId,
          label: d.label,
          groupId: d.groupId
        })));
        
        // ä»®æƒ³ãƒ‡ãƒã‚¤ã‚¹ï¼ˆBlackHoleç­‰ï¼‰ã‚’é™¤å¤–ã—ã¦å®Ÿéš›ã®ãƒã‚¤ã‚¯ã‚’é¸æŠ
        const realMicrophones = audioInputs.filter(device => {
          const label = device.label.toLowerCase();
          return !label.includes('blackhole') && 
                 !label.includes('virtual') &&
                 !label.includes('loopback') &&
                 !label.includes('soundflower') &&
                 !label.includes('aggregate') &&
                 !label.includes('multi-output');
        });
        
        console.log('[WHISPER] Real microphone devices found:', realMicrophones.map(d => ({
          deviceId: d.deviceId,
          label: d.label
        })));
        
        if (realMicrophones.length === 0) {
          throw new Error('å®Ÿéš›ã®ãƒã‚¤ã‚¯ãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\\n\\nğŸ”§ è§£æ±ºæ–¹æ³•:\\n1. ç‰©ç†çš„ãªãƒã‚¤ã‚¯ãŒæ¥ç¶šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\\n2. ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨­å®šã§ãƒã‚¤ã‚¯ãŒèªè­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\\n3. BlackHoleä»¥å¤–ã®ãƒã‚¤ã‚¯ãƒ‡ãƒã‚¤ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„');
        }
        
        // MacBookå†…è”µãƒã‚¤ã‚¯ã‚’å„ªå…ˆçš„ã«é¸æŠ
        let selectedMic = realMicrophones.find(device => {
          const label = device.label.toLowerCase();
          return label.includes('built-in') || 
                 label.includes('internal') ||
                 label.includes('macbook') ||
                 label.includes('default');
        });
        
        // å†…è”µãƒã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€å¤–éƒ¨ãƒ‡ãƒã‚¤ã‚¹ï¼ˆiPhoneç­‰ï¼‰ã‚’é™¤å¤–
        if (!selectedMic) {
          const nonMobileDevices = realMicrophones.filter(device => {
            const label = device.label.toLowerCase();
            return !label.includes('iphone') && 
                   !label.includes('ipad') &&
                   !label.includes('airpods') &&
                   !label.includes('bluetooth');
          });
          
          if (nonMobileDevices.length > 0) {
            selectedMic = nonMobileDevices[0];
          } else {
            selectedMic = realMicrophones[0]; // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
          }
        }
        console.log('[WHISPER] Selected microphone:', selectedMic.label);
        
        stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            deviceId: { exact: selectedMic.deviceId },
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 16000,
            channelCount: 1
          }
        });
        console.log('[WHISPER] âœ… Microphone capture successful with device:', selectedMic.label);
        
        // ãƒã‚¤ã‚¯ã®è©³ç´°æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        const audioTracks = stream.getAudioTracks();
        if (audioTracks.length > 0) {
          const track = audioTracks[0];
          console.log('[WHISPER] ğŸ¤ Microphone track details:', {
            label: track.label,
            enabled: track.enabled,
            muted: track.muted,
            readyState: track.readyState,
            settings: track.getSettings(),
            capabilities: track.getCapabilities()
          });
        }
      } else {
        // ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ä¼šè­°ï¼šBlackHoleã‚’ä½¿ç”¨
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioInputs = devices.filter(device => device.kind === 'audioinput');
        
        const blackHoleDevice = audioInputs.find(device => {
          const label = device.label.toLowerCase();
          return label.includes('blackhole') || 
                 label.includes('virtual') ||
                 label.includes('loopback') ||
                 label.includes('soundflower');
        });
        
        if (!blackHoleDevice) {
          throw new Error('BlackHoleãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\nğŸ”§ è§£æ±ºæ–¹æ³•:\n1. BlackHoleã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n2. Audio MIDIè¨­å®šã§Multi-Output Deviceã‚’ä½œæˆ\n3. BlackHole 2ch + ã‚¤ãƒ¤ãƒ›ãƒ³ã‚’é¸æŠ\n4. ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨­å®šã§å‡ºåŠ›ãƒ‡ãƒã‚¤ã‚¹ã‚’Multi-Output Deviceã«è¨­å®š');
        }
        
        console.log('[WHISPER] Using BlackHole device:', blackHoleDevice.label);
        
        stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            deviceId: { exact: blackHoleDevice.deviceId },
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false,
            sampleRate: 16000,
            channelCount: 1,
          }
        });
        console.log('[WHISPER] âœ… BlackHole audio capture successful');
      }
      
      streamRef.current = stream;
      setIsRecording(true);
      setIsPaused(false); // Reset pause state when starting
      
      // éŸ³å£°ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±
      const audioTracks = stream.getAudioTracks();
      const trackLabel = audioTracks[0]?.label || '';
      
      console.log('[WHISPER] Audio stream obtained:', {
        trackLabel,
        trackCount: audioTracks.length
      });
      
      // éŸ³å£°ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’UIã«é€ä¿¡
      if (onAudioSourceChange) {
        onAudioSourceChange(trackLabel, 0);
      }
      
      if (isInPerson) {
        console.log('[WHISPER] ğŸ¤ Microphone detected! Perfect for in-person meeting');
      } else {
        console.log('[WHISPER] ğŸ‰ BlackHole detected! Perfect for online meeting with system audio');
      }
      
      // éŸ³å£°åˆ†æã®è¨­å®š
      setupAudioAnalysis(stream);

      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm';
        
      const recordAndSend = () => {
        if (!streamRef.current || isPausedRef.current) {
          if (!streamRef.current) {
            console.warn('[WHISPER] No stream available, stopping recording cycle');
            if (intervalRef.current) clearInterval(intervalRef.current);
            setIsRecording(false);
          }
          return;
        }

        console.log('[WHISPER] Starting new recording cycle...');
        const recorder = new MediaRecorder(streamRef.current, { mimeType });
        currentRecorderRef.current = recorder; // ç¾åœ¨ã®recorderã‚’ä¿å­˜
        const chunks: Blob[] = [];

        recorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
            chunks.push(event.data);
            console.log('[WHISPER] Audio chunk received:', event.data.size, 'bytes');
          }
        };

        recorder.onstop = async () => {
          const audioBlob = new Blob(chunks, { type: mimeType });
          console.log('[WHISPER] Recording stopped, total size:', audioBlob.size, 'bytes');
          
          if (audioBlob.size > 1024) {
            console.log('[WHISPER] Sending chunk to Whisper API...');
            await transcribeAudio(audioBlob);
          } else {
            console.warn('[WHISPER] Audio chunk too small, skipping transcription');
          }
          
          // Clear recorder reference when done
          if (currentRecorderRef.current === recorder) {
            currentRecorderRef.current = null;
          }
        };

        recorder.onerror = (event) => {
          console.error('[WHISPER] MediaRecorder error:', event);
          if (onError) {
            onError('éŒ²éŸ³ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ' + event.error);
          }
        };

        recorder.start();
        console.log('[WHISPER] MediaRecorder started, state:', recorder.state);
        
        setTimeout(() => {
          if (recorder.state === 'recording' && !isPaused) {
            console.log('[WHISPER] Stopping recorder after 20 seconds');
            recorder.stop();
          }
        }, 20000); // 20ç§’é–“éŒ²éŸ³
      };

      // æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã™ãã«éŒ²éŸ³ãƒ»é€ä¿¡ã—ã€ãã®å¾Œã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã§ç¹°ã‚Šè¿”ã™
      recordAndSend();
      intervalRef.current = setInterval(recordAndSend, 20100); // 20.1ç§’ã”ã¨ã«ã‚µã‚¤ã‚¯ãƒ«

      console.log('[WHISPER] Recording setup completed successfully');
      return true;
    } catch (error) {
      console.error('[WHISPER] Start recording error:', error);
      if (onError) {
        if (error instanceof DOMException) {
          switch (error.name) {
            case 'NotAllowedError':
              onError('éŸ³å£°ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã§éŸ³å£°ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã—ã¦ãã ã•ã„ã€‚');
              break;
            case 'NotFoundError':
              onError('éŸ³å£°ãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã¾ãŸã¯ãƒã‚¤ã‚¯ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
              break;
            case 'NotSupportedError':
              onError('éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚');
              break;
            case 'AbortError':
              onError('éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚');
              break;
            default:
              onError(`éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚¨ãƒ©ãƒ¼: ${error.name} - ${error.message}`);
          }
        } else {
          onError(error instanceof Error ? error.message : 'éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ');
        }
      }
      setIsRecording(false);
      return false;
    }
  }, [isSupported, isRecording, isPaused, transcribeAudio, onError, setupAudioAnalysis, meetingType, onAudioSourceChange]);

  const pauseRecording = useCallback(() => {
    if (!isRecording) return;
    
    console.log('[WHISPER] Pausing recording...');
    setIsPaused(true);
    isPausedRef.current = true;
    
    // Stop current recorder if it's running
    if (currentRecorderRef.current && currentRecorderRef.current.state === 'recording') {
      console.log('[WHISPER] Stopping current recorder for pause');
      currentRecorderRef.current.stop();
    }
    
    // Clear the recording interval to prevent automatic resumption
    if (intervalRef.current) {
      console.log('[WHISPER] Clearing recording interval for pause');
      clearInterval(intervalRef.current);
      intervalRef.current = null;
    }
    
    console.log('[WHISPER] Recording paused successfully');
  }, [isRecording]);

  const resumeRecording = useCallback(() => {
    if (!isRecording || !isPaused) return;
    
    console.log('[WHISPER] Resuming recording...');
    setIsPaused(false);
    isPausedRef.current = false;
    
    // Restart the recording interval using the original recordAndSend function
    if (!intervalRef.current && streamRef.current) {
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm';
        
      const recordAndSend = () => {
        if (!streamRef.current) {
          console.warn('[WHISPER] No stream available, stopping recording cycle');
          if (intervalRef.current) clearInterval(intervalRef.current);
          setIsRecording(false);
          return;
        }

        console.log('[WHISPER] Resume: Starting new recording cycle...');
        const recorder = new MediaRecorder(streamRef.current, { mimeType });
        currentRecorderRef.current = recorder;
        const chunks: Blob[] = [];

        recorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            chunks.push(event.data);
          }
        };

        recorder.onstop = async () => {
          if (chunks.length > 0) {
            const audioBlob = new Blob(chunks, { type: mimeType });
            await transcribeAudio(audioBlob);
          }
        };

        recorder.start();
        setTimeout(() => {
          if (recorder.state === 'recording') {
            recorder.stop();
          }
        }, 20000);
      };
      
      // Start recording immediately and set up interval
      setTimeout(() => {
        recordAndSend();
        intervalRef.current = setInterval(recordAndSend, 20100);
      }, 100); // Small delay to ensure isPaused state is updated
      
      console.log('[WHISPER] Recording interval restarted for resume');
    }
    
    console.log('[WHISPER] Recording resumed successfully');
  }, [isRecording, isPaused, transcribeAudio]);

  const stopRecording = useCallback(async () => {
    console.log('[WHISPER] Stopping recording...');
    setIsRecording(false); // ã“ã‚Œã‚’æœ€åˆã«å‘¼ã¶ã“ã¨ã§ã€ã‚µã‚¤ã‚¯ãƒ«ã‚’åœæ­¢
    setIsPaused(false); // Reset pause state
    isPausedRef.current = false;
    
    if (intervalRef.current) {
      console.log('[WHISPER] Clearing recording interval');
      clearInterval(intervalRef.current);
      intervalRef.current = null;
      }

    // Stop current recorder if running
    if (currentRecorderRef.current && currentRecorderRef.current.state === 'recording') {
      console.log('[WHISPER] Stopping current recorder');
      currentRecorderRef.current.stop();
    }
    currentRecorderRef.current = null;

      if (streamRef.current) {
      console.log('[WHISPER] Stopping audio tracks');
      streamRef.current.getTracks().forEach((track) => {
        console.log('[WHISPER] Stopping track:', track.kind, track.label);
        track.stop();
      });
        streamRef.current = null;
      }

    console.log('[WHISPER] Recording stopped successfully');
  }, []);

  return {
    isRecording,
    isPaused,
    isSupported,
    startRecording,
    stopRecording,
    pauseRecording,
    resumeRecording
  };
}