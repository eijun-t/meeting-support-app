'use client';

import { useState, useEffect, useRef, useCallback } from 'react';

interface TranscriptionEntry {
  id: string;
  text: string;
  timestamp: Date;
  speaker?: string;
}

interface WhisperTranscriptionProps {
  onTranscription?: (entry: TranscriptionEntry) => void;
  onError?: (error: string) => void;
  onAudioSourceChange?: (source: string, level: number) => void;
  apiKey?: string;
}

export function useWhisperTranscription({ onTranscription, onError, onAudioSourceChange, apiKey }: WhisperTranscriptionProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [isSupported, setIsSupported] = useState(false);
  const streamRef = useRef<MediaStream | null>(null);
  const intervalRef = useRef<NodeJS.Timeout | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const audioLevelRef = useRef<number>(0);
  const genericResponseCountRef = useRef<number>(0);
  const lastTranscriptionTimeRef = useRef<number>(0);

  useEffect(() => {
    setIsSupported(true);
    
    return () => {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, []);

  // éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–ã®è¨­å®š
  const setupAudioAnalysis = useCallback((stream: MediaStream) => {
    try {
      const audioContext = new (window.AudioContext || (window as unknown as { webkitAudioContext: typeof AudioContext }).webkitAudioContext)();
      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(stream);
      
      analyser.fftSize = 256;
      source.connect(analyser);
      
      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      
      console.log('[WHISPER] Audio analysis setup complete');
      
      // éŸ³å£°ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
      const trackLabel = stream.getAudioTracks()[0]?.label || '';
      
      // éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–ãƒ«ãƒ¼ãƒ—
      const monitorAudioLevel = () => {
        if (analyserRef.current) {
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          analyser.getByteFrequencyData(dataArray);
          
          // å¹³å‡éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
          const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
          audioLevelRef.current = average;
          
          // éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’UIã«é€ä¿¡ï¼ˆ1ç§’ã«ç´„2å›ç¨‹åº¦ï¼‰
          if (Math.random() < 0.02 && onAudioSourceChange) {
            onAudioSourceChange(trackLabel, average);
          }
          
          // éŸ³å£°ãƒ¬ãƒ™ãƒ«ãŒç¶™ç¶šçš„ã«0ã®å ´åˆã®è­¦å‘Šï¼ˆBlackHoleè¨­å®šä¸å‚™ã®å¯èƒ½æ€§ï¼‰
          if (average < 1 && trackLabel.toLowerCase().includes('blackhole')) {
            // BlackHoleãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ã®ã«éŸ³å£°ãƒ¬ãƒ™ãƒ«ãŒ0ã®å ´åˆ
            const now = Date.now();
            if (now - lastTranscriptionTimeRef.current > 60000) { // 1åˆ†é–“éŸ³å£°ãªã—
              if (Math.random() < 0.001) { // ç´„0.1%ã®ç¢ºç‡ï¼ˆ10ç§’ã«1å›ç¨‹åº¦ï¼‰
                console.warn('[WHISPER] ğŸ”´ BlackHoleãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ãŒéŸ³å£°ãƒ¬ãƒ™ãƒ«ãŒ0ã§ã™');
                console.warn('[WHISPER] ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„:');
                console.warn('[WHISPER] 1. ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨­å®š > ã‚µã‚¦ãƒ³ãƒ‰ > å‡ºåŠ›ã§ã€ŒMulti-Output Deviceã€ã‚’é¸æŠ');
                console.warn('[WHISPER] 2. ã¾ãŸã¯éŸ³å£°è¨­å®šã§ã€Œç”»é¢å…±æœ‰ã€ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ');
              }
            }
          }
          
          // é€šå¸¸ã®éŸ³å£°ãƒ¬ãƒ™ãƒ«ãƒ­ã‚°ï¼ˆ5ç§’ã”ã¨ï¼‰
          if (Math.random() < 0.01) { // ç´„1%ã®ç¢ºç‡ã§ãƒ­ã‚°å‡ºåŠ›ï¼ˆ5ç§’ã«1å›ç¨‹åº¦ï¼‰
            console.log('[WHISPER] Audio level:', Math.round(average), 'Max:', Math.max(...dataArray));
          }
        }
        
        if (isRecording) {
          requestAnimationFrame(monitorAudioLevel);
        }
      };
      
      monitorAudioLevel();
      
    } catch (error) {
      console.error('[WHISPER] Audio analysis setup failed:', error);
    }
  }, [isRecording, onAudioSourceChange]);

  // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ
  const analyzeAudioBlob = useCallback(async (audioBlob: Blob): Promise<void> => {
    console.log('[WHISPER] === AUDIO ANALYSIS START ===');
    console.log('[WHISPER] Blob size:', audioBlob.size, 'bytes');
    console.log('[WHISPER] Blob type:', audioBlob.type);
    console.log('[WHISPER] Current audio level:', Math.round(audioLevelRef.current));
    
    // ArrayBufferã«å¤‰æ›ã—ã¦æœ€åˆã¨æœ€å¾Œã®æ•°ãƒã‚¤ãƒˆã‚’ç¢ºèª
    try {
      const arrayBuffer = await audioBlob.arrayBuffer();
      const uint8Array = new Uint8Array(arrayBuffer);
      
      console.log('[WHISPER] First 10 bytes:', Array.from(uint8Array.slice(0, 10)).map(b => b.toString(16).padStart(2, '0')).join(' '));
      console.log('[WHISPER] Last 10 bytes:', Array.from(uint8Array.slice(-10)).map(b => b.toString(16).padStart(2, '0')).join(' '));
      
      // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ
      const nonZeroBytes = uint8Array.filter(b => b !== 0).length;
      const silenceRatio = (uint8Array.length - nonZeroBytes) / uint8Array.length;
      
      console.log('[WHISPER] Non-zero bytes:', nonZeroBytes, '/', uint8Array.length);
      console.log('[WHISPER] Silence ratio:', Math.round(silenceRatio * 100) + '%');
      
      if (silenceRatio > 0.9) {
        console.warn('[WHISPER] âš ï¸ HIGH SILENCE RATIO - Audio might be too quiet!');
        console.warn('[WHISPER] ğŸ’¡ TIPS:');
        console.warn('[WHISPER]   1. Make sure system audio is actually playing');
        console.warn('[WHISPER]   2. When browser asks for screen sharing, check "Share audio" option');
        console.warn('[WHISPER]   3. Select the correct application/tab that is playing audio');
        console.warn('[WHISPER]   4. Increase system volume if audio is too quiet');
      }
      
      if (audioLevelRef.current < 3) {
        console.warn('[WHISPER] âš ï¸ VERY LOW AUDIO LEVEL:', Math.round(audioLevelRef.current));
        console.warn('[WHISPER] This might indicate microphone input instead of system audio!');
        
        // éŸ³å£°ãƒ¬ãƒ™ãƒ«ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ–
        // ï¼ˆBlackHoleç­‰ã®ä»®æƒ³ãƒ‡ãƒã‚¤ã‚¹ä½¿ç”¨æ™‚ã«èª¤æ¤œå‡ºãŒç™ºç”Ÿã™ã‚‹ãŸã‚ï¼‰
        // å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒ­ã‚°ã§ç¢ºèªå¯èƒ½
      }
      
      // éŸ³å£°ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¨˜éŒ²ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯å‰Šé™¤ï¼‰
      console.log('[WHISPER] Audio blob analysis completed');
      
    } catch (error) {
      console.error('[WHISPER] Audio analysis failed:', error);
    }
    
    console.log('[WHISPER] === AUDIO ANALYSIS END ===');
  }, []);

  const transcribeAudio = useCallback(async (audioBlob: Blob) => {
    console.log('[WHISPER] transcribeAudio called with blob size:', audioBlob.size);
    
    // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æã‚’å®Ÿè¡Œ
    await analyzeAudioBlob(audioBlob);
    
    if (!apiKey) {
      console.error('[WHISPER] No API key available');
      if (onError) onError('OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã‹ã‚‰è¿½åŠ ã—ã¦ãã ã•ã„ã€‚');
      return;
    }

    try {
      const formData = new FormData();
      // é€ä¿¡ã™ã‚‹éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã«æ­£ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã¨MIMEã‚¿ã‚¤ãƒ—ã‚’è¨­å®š
      const fileName = 'audio.webm';
      
      formData.append('file', audioBlob, fileName);
      formData.append('model', 'whisper-1');
      formData.append('language', 'ja');
      formData.append('response_format', 'json');
      
      console.log('[WHISPER] Sending request to Whisper API with data:', {
        fileSize: audioBlob.size,
        fileName,
        model: 'whisper-1',
        language: 'ja'
      });

      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
        },
        body: formData,
      });
      
      console.log('[WHISPER] API response status:', response.status);

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`API Error ${response.status}:`, errorText);
        
        if (response.status === 401) {
          throw new Error('APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚æ­£ã—ã„OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚');
        } else if (response.status === 429) {
          throw new Error('APIä½¿ç”¨é‡ã®åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚');
        } else if (response.status === 400) {
          throw new Error(`éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã§ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ${audioBlob.size}ãƒã‚¤ãƒˆ`);
        } else {
          throw new Error(`API ã‚¨ãƒ©ãƒ¼: ${response.status} - ${errorText}`);
        }
      }

      const result = await response.json();
      console.log('[WHISPER] API response result:', result);
      
      if (result.text && result.text.trim()) {
        const transcribedText = result.text.trim();
        
        // å…¸å‹çš„ãªã€Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€å¿œç­”ã‚’æ¤œå‡º
        const genericResponses = [
          'ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
          'Thank you for watching',
          'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ',
          '...',
          'ã€‚',
          'ã†ã‚“',
          'ã¯ã„',
          'ãã†ã§ã™ã­'
        ];
        
        const isGenericResponse = genericResponses.some(generic => transcribedText === generic);
        
        if (isGenericResponse) {
          genericResponseCountRef.current++;
          console.warn('[WHISPER] âš ï¸ GENERIC RESPONSE DETECTED:', transcribedText, `(${genericResponseCountRef.current} consecutive)`);
          
          if (genericResponseCountRef.current >= 3) {
            console.error('[WHISPER] ğŸš¨ 3é€£ç¶šã§Generic Response! ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ãŒã‚­ãƒ£ãƒ—ãƒãƒ£ã•ã‚Œã¦ã„ã¾ã›ã‚“');
            if (onError) {
              onError('âš ï¸ ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ãŒã‚­ãƒ£ãƒ—ãƒãƒ£ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚éŸ³å£°è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\nğŸ”§ è§£æ±ºæ–¹æ³•:\nâ€¢ BlackHoleä½¿ç”¨æ™‚: ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨­å®šã§ã€ŒMulti-Output Deviceã€ã‚’é¸æŠ\nâ€¢ Multi-Output Deviceã§BlackHole 2ch + ã‚¤ãƒ¤ãƒ›ãƒ³ã‚’ä¸¡æ–¹é¸æŠ\nâ€¢ ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨­å®šã§å‡ºåŠ›ãƒ‡ãƒã‚¤ã‚¹ã‚’Multi-Output Deviceã«è¨­å®š');
            }
            // éŒ²éŸ³ã‚’åœæ­¢ã—ã¦å†è¨­å®šã‚’ä¿ƒã™
            return;
          }
        } else {
          // æ­£å¸¸ãªå¿œç­”ãŒå¾—ã‚‰ã‚ŒãŸå ´åˆã¯ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
          genericResponseCountRef.current = 0;
          lastTranscriptionTimeRef.current = Date.now(); // æ­£å¸¸ãªè»¢å†™æ™‚åˆ»ã‚’è¨˜éŒ²
        }
        
        if (onTranscription) {
        const entry: TranscriptionEntry = {
          id: Date.now().toString(),
            text: transcribedText,
          timestamp: new Date(),
          speaker: 'è©±è€…'
        };
          console.log('[WHISPER] Transcription result:', entry.text);
        onTranscription(entry);
        }
      } else {
        console.log('[WHISPER] No transcription text received or text is empty');
      }
    } catch (error) {
      console.error('[WHISPER] Transcription error:', error);
      if (onError) {
        onError(error instanceof Error ? error.message : 'Whisper APIã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ');
      }
    }
  }, [apiKey, onTranscription, onError, analyzeAudioBlob]);

  const startRecording = useCallback(async () => {
    if (!isSupported || isRecording) return false;
    
    console.log('[WHISPER] Starting BlackHole recording...');

    try {
      let stream = null;
      
      // BlackHoleãƒ‡ãƒã‚¤ã‚¹ã‚’æ¢ã™
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputs = devices.filter(device => device.kind === 'audioinput');
      
      const blackHoleDevice = audioInputs.find(device => {
        const label = device.label.toLowerCase();
        return label.includes('blackhole') || 
               label.includes('virtual') ||
               label.includes('loopback') ||
               label.includes('soundflower');
      });
      
      if (!blackHoleDevice) {
        throw new Error('BlackHoleãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\nğŸ”§ è§£æ±ºæ–¹æ³•:\n1. BlackHoleã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n2. Audio MIDIè¨­å®šã§Multi-Output Deviceã‚’ä½œæˆ\n3. BlackHole 2ch + ã‚¤ãƒ¤ãƒ›ãƒ³ã‚’é¸æŠ\n4. ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨­å®šã§å‡ºåŠ›ãƒ‡ãƒã‚¤ã‚¹ã‚’Multi-Output Deviceã«è¨­å®š');
      }
      
      console.log('[WHISPER] Using BlackHole device:', blackHoleDevice.label);
      
      stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          deviceId: { exact: blackHoleDevice.deviceId },
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: 16000,
          channelCount: 1,
        }
      });
      
      console.log('[WHISPER] âœ… BlackHole audio capture successful');
      
      streamRef.current = stream;
      setIsRecording(true);
      
      // éŸ³å£°ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±
      const audioTracks = stream.getAudioTracks();
      const trackLabel = audioTracks[0]?.label || '';
      
      console.log('[WHISPER] Audio stream obtained:', {
        trackLabel,
        trackCount: audioTracks.length
      });
      
      // éŸ³å£°ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’UIã«é€ä¿¡
      if (onAudioSourceChange) {
        onAudioSourceChange(trackLabel, 0);
      }
      
      console.log('[WHISPER] ğŸ‰ BlackHole detected! Perfect for headphone + system audio setup');
      
      // éŸ³å£°åˆ†æã®è¨­å®š
      setupAudioAnalysis(stream);

      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm';
        
      const recordAndSend = () => {
        if (!streamRef.current) {
          console.warn('[WHISPER] No stream available, stopping recording cycle');
          if (intervalRef.current) clearInterval(intervalRef.current);
          setIsRecording(false);
          return;
        }

        console.log('[WHISPER] Starting new recording cycle...');
        const recorder = new MediaRecorder(streamRef.current, { mimeType });
        const chunks: Blob[] = [];

        recorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
            chunks.push(event.data);
            console.log('[WHISPER] Audio chunk received:', event.data.size, 'bytes');
          }
        };

        recorder.onstop = async () => {
          const audioBlob = new Blob(chunks, { type: mimeType });
          console.log('[WHISPER] Recording stopped, total size:', audioBlob.size, 'bytes');
          
          if (audioBlob.size > 1024) {
            console.log('[WHISPER] Sending chunk to Whisper API...');
            await transcribeAudio(audioBlob);
          } else {
            console.warn('[WHISPER] Audio chunk too small, skipping transcription');
          }
        };

        recorder.onerror = (event) => {
          console.error('[WHISPER] MediaRecorder error:', event);
          if (onError) {
            onError('éŒ²éŸ³ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ' + event.error);
          }
        };

        recorder.start();
        console.log('[WHISPER] MediaRecorder started, state:', recorder.state);
        
        setTimeout(() => {
          if (recorder.state === 'recording') {
            console.log('[WHISPER] Stopping recorder after 20 seconds');
            recorder.stop();
          }
        }, 20000); // 20ç§’é–“éŒ²éŸ³
      };

      // æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã™ãã«éŒ²éŸ³ãƒ»é€ä¿¡ã—ã€ãã®å¾Œã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã§ç¹°ã‚Šè¿”ã™
      recordAndSend();
      intervalRef.current = setInterval(recordAndSend, 20100); // 20.1ç§’ã”ã¨ã«ã‚µã‚¤ã‚¯ãƒ«

      console.log('[WHISPER] Recording setup completed successfully');
      return true;
    } catch (error) {
      console.error('[WHISPER] Start recording error:', error);
      if (onError) {
        if (error instanceof DOMException) {
          switch (error.name) {
            case 'NotAllowedError':
              onError('éŸ³å£°ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã§éŸ³å£°ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã—ã¦ãã ã•ã„ã€‚');
              break;
            case 'NotFoundError':
              onError('éŸ³å£°ãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã¾ãŸã¯ãƒã‚¤ã‚¯ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
              break;
            case 'NotSupportedError':
              onError('éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚');
              break;
            case 'AbortError':
              onError('éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚');
              break;
            default:
              onError(`éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚¨ãƒ©ãƒ¼: ${error.name} - ${error.message}`);
          }
        } else {
          onError(error instanceof Error ? error.message : 'éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ');
        }
      }
      setIsRecording(false);
      return false;
    }
  }, [isSupported, isRecording, transcribeAudio, onError, setupAudioAnalysis]);

  const stopRecording = useCallback(async () => {
    console.log('[WHISPER] Stopping recording...');
    setIsRecording(false); // ã“ã‚Œã‚’æœ€åˆã«å‘¼ã¶ã“ã¨ã§ã€ã‚µã‚¤ã‚¯ãƒ«ã‚’åœæ­¢
    
    if (intervalRef.current) {
      console.log('[WHISPER] Clearing recording interval');
      clearInterval(intervalRef.current);
      intervalRef.current = null;
      }

      if (streamRef.current) {
      console.log('[WHISPER] Stopping audio tracks');
      streamRef.current.getTracks().forEach((track) => {
        console.log('[WHISPER] Stopping track:', track.kind, track.label);
        track.stop();
      });
        streamRef.current = null;
      }

    console.log('[WHISPER] Recording stopped successfully');
  }, []);

  return {
    isRecording,
    isSupported,
    startRecording,
    stopRecording
  };
}